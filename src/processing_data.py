import pandas as pd
import os
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import joblib

def process_data(input_path, output_folder="data/processed", model_dir='models'):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu: Map Target t·ª´ Ch·ªØ sang S·ªë & Feature Engineering
    """
    
    # --- 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu ---
    if not os.path.exists(input_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {input_path}")
        return

    df = pd.read_csv(input_path)
    print(f"üìä T·ªïng s·ªë m·∫´u: {len(df)}")
    
    target_col = 'RF Link Quality'

    # --- 2Ô∏è‚É£ MAP TARGET (Ch·ªØ -> S·ªë) ---
    print("üîÑ ƒêang chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng s·ªë...")
    quality_map = {'Poor': 0, 'Moderate': 1, 'Good': 2}
    
    # Map v√† x·ª≠ l√Ω l·ªói n·∫øu c√≥ gi√° tr·ªã l·∫°
    df[target_col] = df[target_col].map(quality_map)
    
    # Ki·ªÉm tra xem c√≥ d√≤ng n√†o b·ªã NaN (do l·ªói ch√≠nh t·∫£ trong file raw) kh√¥ng
    if df[target_col].isnull().any():
        print("‚ö†Ô∏è C·∫£nh b√°o: C√≥ nh√£n kh√¥ng h·ª£p l·ªá, ƒëang lo·∫°i b·ªè...")
        df.dropna(subset=[target_col], inplace=True)
        
    df[target_col] = df[target_col].astype(int)
    print(f"‚úÖ Ph√¢n ph·ªëi sau khi map: {df[target_col].value_counts().to_dict()}")

    # --- 3Ô∏è‚É£ Map Congestion (Ch·ªØ -> S·ªë ƒë·ªÉ t√≠nh to√°n) ---
    congestion_map = {'Low': 1, 'Medium': 2, 'High': 3}
    df['Network Congestion Score'] = df['Network Congestion'].map(congestion_map).fillna(2).astype(int)

    # --- 4Ô∏è‚É£ Feature Engineering ---
    print("üõ†Ô∏è ƒêang t·∫°o c√°c features m·ªõi...")
    
    df['Mobility_Impact'] = df['User Speed (m/s)'] * (df['Handover Events'] + 1)
    df['Signal_Quality_Index'] = df['Signal Strength (dBm)'] * df['Network Congestion Score']
    df['Device_Stress_Level'] = df['Power Consumption (mW)'] / (df['Battery Level (%)'] + 1)
    df['Log_Distance'] = np.log1p(df['Distance from Base Station (m)'])

    # --- 5Ô∏è‚É£ L·ªçc b·ªè Columns ---
    # B·ªè Throughput, Latency (ƒê√°p √°n) v√† c√°c c·ªôt text g·ªëc
    cols_to_drop = [
        'Throughput (Mbps)', 
        'Latency (ms)', 
        'Network Congestion', # B·ªè c·ªôt ch·ªØ, gi·ªØ c·ªôt Score
        target_col # T√°ch target ri√™ng
    ]
    
    X = df.drop(columns=cols_to_drop)
    y = df[target_col]
    
    print(f"\nüîç Features d√πng ƒë·ªÉ train ({len(X.columns)}): {list(X.columns)}")
    
    # --- 6Ô∏è‚É£ Scaling ---
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    X_processed = pd.DataFrame(X_scaled, columns=X.columns)
    
    # G√°n l·∫°i target
    df_processed = X_processed.copy()
    df_processed[target_col] = y.values
    
    # --- 7Ô∏è‚É£ Chia Train/Test ---
    train_df, test_df = train_test_split(
        df_processed, 
        test_size=0.2, 
        random_state=42, 
        stratify=df_processed[target_col]
    )
    
    # --- 8Ô∏è‚É£ L∆∞u k·∫øt qu·∫£ ---
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    train_df.to_csv(os.path.join(output_folder, "train.csv"), index=False)
    test_df.to_csv(os.path.join(output_folder, "test.csv"), index=False)
    
    # L∆∞u metadata
    joblib.dump(scaler, os.path.join(model_dir, "minmax_scaler.pkl"))
    
    feature_info = {
        'numerical_features': list(X.columns),
        'categorical_features': [],
        'all_features': list(X.columns) + [target_col]
    }
    joblib.dump(feature_info, os.path.join(model_dir, "feature_info.pkl"))
    
    print(f"\n‚úÖ X·ª≠ l√Ω ho√†n t·∫•t!")
    return train_df, test_df

if __name__ == "__main__":
    process_data("data/raw/wireless_communication_dataset.csv")