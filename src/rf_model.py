import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.preprocessing import label_binarize
import joblib
import os

def train_and_evaluate_final(
    train_path="data/synthetic/train_smote_balanced.csv", 
    test_path="data/processed/test.csv",
    model_dir="models",
    report_dir="reports"
):
    print("ğŸš€ Báº¯t Ä‘áº§u quy trÃ¬nh huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­ cuá»‘i cÃ¹ng...")
    
    # --- 1ï¸âƒ£ Load dá»¯ liá»‡u ---
    if not os.path.exists(train_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {train_path}")
        return
    df_train = pd.read_csv(train_path)
    
    if not os.path.exists(test_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {test_path}")
        return
    df_test = pd.read_csv(test_path)

    print(f"ğŸ“Š Dá»¯ liá»‡u Train (Synthetic): {len(df_train)} máº«u")
    print(f"ğŸ“Š Dá»¯ liá»‡u Test (Real): {len(df_test)} máº«u")
    
    target_col = 'RF Link Quality'
    class_names = ['Poor', 'Moderate', 'Good']
    class_map = {'Poor': 0, 'Moderate': 1, 'Good': 2}
    
    X_train = df_train.drop(columns=[target_col])
    y_train = df_train[target_col]
    
    X_test = df_test.drop(columns=[target_col])
    y_test = df_test[target_col]
    
    # --- 2ï¸âƒ£ Huáº¥n luyá»‡n Model ---
    print("\nğŸ¤– Äang train model Random Forest...")
    rf_final = RandomForestClassifier(
        n_estimators=500, 
        random_state=42, 
        n_jobs=-1,
        class_weight='balanced_subsample',
        max_depth=20,
        max_features='log2',
        min_samples_leaf=1,
        min_samples_split=2,
        criterion='gini'
    )
    rf_final.fit(X_train, y_train)
    
    # LÆ°u Model
    os.makedirs(model_dir, exist_ok=True)
    joblib.dump(rf_final, os.path.join(model_dir, "rf_final_model.pkl"))
    
    # --- 3ï¸âƒ£ Dá»± Ä‘oÃ¡n ---
    print("\nâš–ï¸ Äang Ä‘Ã¡nh giÃ¡ trÃªn táº­p Test thá»±c táº¿...")
    y_pred = rf_final.predict(X_test)
    y_pred_proba = rf_final.predict_proba(X_test)
    
    # --- 4ï¸âƒ£ TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ chi tiáº¿t ---
    print("\nğŸ“Š TÃNH TOÃN CÃC CHá»ˆ Sá» ÄÃNH GIÃ:")
    print("=" * 50)
    
    # 4.1 Accuracy tá»•ng thá»ƒ
    acc = accuracy_score(y_test, y_pred)
    print(f"âœ… Accuracy (Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ): {acc:.4f} ({acc:.2%})")
    
    # 4.2 Precision, Recall, F1 cho tá»«ng lá»›p
    precision_per_class = precision_score(y_test, y_pred, average=None)
    recall_per_class = recall_score(y_test, y_pred, average=None)
    f1_per_class = f1_score(y_test, y_pred, average=None)
    
    print(f"\nğŸ“ˆ CHá»ˆ Sá» THEO Tá»ªNG Lá»šP:")
    print("-" * 60)
    print(f"{'Lá»›p':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
    print("-" * 60)
    
    for i, class_name in enumerate(class_names):
        # Äáº¿m sá»‘ máº«u thá»±c táº¿ cá»§a má»—i lá»›p
        support = np.sum(y_test == i)
        print(f"{class_name:<10} {precision_per_class[i]:<12.4f} {recall_per_class[i]:<12.4f} "
              f"{f1_per_class[i]:<12.4f} {support:<10}")
    
    # 4.3 Macro vÃ  Weighted Average
    precision_macro = precision_score(y_test, y_pred, average='macro')
    recall_macro = recall_score(y_test, y_pred, average='macro')
    f1_macro = f1_score(y_test, y_pred, average='macro')
    
    precision_weighted = precision_score(y_test, y_pred, average='weighted')
    recall_weighted = recall_score(y_test, y_pred, average='weighted')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nğŸ“Š MACRO AVERAGE:")
    print(f"  Precision: {precision_macro:.4f} | Recall: {recall_macro:.4f} | F1-Score: {f1_macro:.4f}")
    
    print(f"ğŸ“Š WEIGHTED AVERAGE:")
    print(f"  Precision: {precision_weighted:.4f} | Recall: {recall_weighted:.4f} | F1-Score: {f1_weighted:.4f}")
    
    # 4.4 AUC-ROC (cho multi-class)
    try:
        # Binarize labels cho AUC-ROC
        y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
        auc_roc = roc_auc_score(y_test_bin, y_pred_proba, average='macro', multi_class='ovr')
        print(f"\nğŸ“ˆ AUC-ROC Score (macro, One-vs-Rest): {auc_roc:.4f}")
    except Exception as e:
        print(f"\nâš ï¸ KhÃ´ng thá»ƒ tÃ­nh AUC-ROC: {e}")
    
    # 4.5 Classification Report Ä‘áº§y Ä‘á»§
    print(f"\nğŸ“ CLASSIFICATION REPORT Äáº¦Y Äá»¦:")
    print("-" * 60)
    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))
    
    # --- 5ï¸âƒ£ LÆ°u káº¿t quáº£ vÃ o file text ---
    os.makedirs(report_dir, exist_ok=True)
    
    report_content = f"""
=== Káº¾T QUáº¢ ÄÃNH GIÃ MÃ” HÃŒNH RANDOM FOREST ===
NgÃ y Ä‘Ã¡nh giÃ¡: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
MÃ´ hÃ¬nh: RandomForestClassifier (n_estimators=500, max_depth=20)
Dá»¯ liá»‡u huáº¥n luyá»‡n: {train_path} ({len(df_train)} máº«u)
Dá»¯ liá»‡u kiá»ƒm thá»­: {test_path} ({len(df_test)} máº«u)

=== THá»NG KÃŠ Dá»® LIá»†U ===
Táº­p Train:
{y_train.value_counts().sort_index().to_string()}

Táº­p Test:
{y_test.value_counts().sort_index().to_string()}

=== CHá»ˆ Sá» ÄÃNH GIÃ CHI TIáº¾T ===
1. Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ (Accuracy): {acc:.4f} ({acc:.2%})

2. Chá»‰ sá»‘ theo tá»«ng lá»›p:
{'Lá»›p':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}
{'-'*60}
"""
    
    for i, class_name in enumerate(class_names):
        support = np.sum(y_test == i)
        report_content += f"{class_name:<10} {precision_per_class[i]:<12.4f} {recall_per_class[i]:<12.4f} " \
                         f"{f1_per_class[i]:<12.4f} {support:<10}\n"
    
    report_content += f"""
3. Chá»‰ sá»‘ tá»•ng há»£p:
- Macro Average:
  * Precision: {precision_macro:.4f}
  * Recall: {recall_macro:.4f}
  * F1-Score: {f1_macro:.4f}

- Weighted Average:
  * Precision: {precision_weighted:.4f}
  * Recall: {recall_weighted:.4f}
  * F1-Score: {f1_weighted:.4f}

4. AUC-ROC Score (macro, OvR): {auc_roc if 'auc_roc' in locals() else 'N/A'}

=== MA TRáº¬N NHáº¦M LáºªN ===
{confusion_matrix(y_test, y_pred)}

=== THÃ”NG TIN MÃ” HÃŒNH ===
- Sá»‘ cÃ¢y: 500
- Äá»™ sÃ¢u tá»‘i Ä‘a: 20
- Sá»‘ lÆ°á»£ng features: {X_train.shape[1]}
- Äáº·c trÆ°ng quan trá»ng nháº¥t: {X_train.columns[np.argmax(rf_final.feature_importances_)]}
- Overfit gap: {rf_final.score(X_train, y_train) - acc:.4f}

=== Káº¾T LUáº¬N ===
MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c {acc:.2%} trÃªn táº­p kiá»ƒm thá»­.
"""
    
    # LÆ°u file bÃ¡o cÃ¡o
    report_path = os.path.join(report_dir, "model_performance_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    print(f"\nğŸ“„ ÄÃ£ lÆ°u bÃ¡o cÃ¡o chi tiáº¿t táº¡i: {report_path}")
    
    # --- 6ï¸âƒ£ Váº½ vÃ  lÆ°u Confusion Matrix ---
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, 
                yticklabels=class_names,
                cbar_kws={'label': 'Sá»‘ lÆ°á»£ng máº«u'})
    
    plt.title(f'Confusion Matrix - Random Forest\nAccuracy: {acc:.2%}', fontsize=14, fontweight='bold')
    plt.ylabel('NhÃ£n thá»±c táº¿', fontsize=12)
    plt.xlabel('NhÃ£n dá»± Ä‘oÃ¡n', fontsize=12)
    plt.tight_layout()
    
    cm_path = os.path.join(report_dir, "confusion_matrix_final.png")
    plt.savefig(cm_path, dpi=300)
    print(f"ğŸ“Š ÄÃ£ lÆ°u Confusion Matrix táº¡i: {cm_path}")
    
    # --- 7ï¸âƒ£ Váº½ Feature Importance ---
    importances = rf_final.feature_importances_
    fi_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    bars = plt.barh(range(len(fi_df)), fi_df['Importance'], align='center', color='steelblue')
    plt.yticks(range(len(fi_df)), fi_df['Feature'])
    plt.xlabel('Má»©c Ä‘á»™ quan trá»ng', fontsize=12)
    plt.title('Feature Importance trong Random Forest', fontsize=14, fontweight='bold')
    
    # ThÃªm giÃ¡ trá»‹ sá»‘ trÃªn má»—i bar
    for i, (bar, importance) in enumerate(zip(bars, fi_df['Importance'])):
        plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, 
                f'{importance:.3f}', va='center', fontsize=10)
    
    plt.gca().invert_yaxis()  # Äáº£o ngÆ°á»£c Ä‘á»ƒ feature quan trá»ng nháº¥t á»Ÿ trÃªn
    plt.tight_layout()
    
    fi_path = os.path.join(report_dir, "feature_importance_final.png")
    plt.savefig(fi_path, dpi=300)
    print(f"ğŸ“Š ÄÃ£ lÆ°u Feature Importance táº¡i: {fi_path}")
    
    # --- 8ï¸âƒ£ Táº¡o báº£ng tá»•ng há»£p metrics ---
    metrics_df = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1-Score (Macro)', 'AUC-ROC'],
        'Value': [acc, precision_macro, recall_macro, f1_macro, auc_roc if 'auc_roc' in locals() else np.nan]
    })
    
    metrics_csv_path = os.path.join(report_dir, "model_metrics.csv")
    metrics_df.to_csv(metrics_csv_path, index=False)
    print(f"ğŸ“ˆ ÄÃ£ lÆ°u metrics vÃ o CSV: {metrics_csv_path}")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ HOÃ€N Táº¤T QUY TRÃŒNH ÄÃNH GIÃ!")
    print(f"{'='*60}")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin tá»•ng káº¿t
    print(f"\nğŸ“‹ Tá»”NG Káº¾T Káº¾T QUáº¢:")
    print(f"  â€¢ Äá»™ chÃ­nh xÃ¡c: {acc:.2%}")
    print(f"  â€¢ F1-Score (Macro): {f1_macro:.2%}")
    print(f"  â€¢ Recall lá»›p Good: {recall_per_class[2]:.2%}")
    print(f"  â€¢ Model Ä‘Ã£ lÆ°u táº¡i: models/rf_final_model.pkl")
    
    return rf_final, {
        'accuracy': acc,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'f1_per_class': f1_per_class
    }

if __name__ == "__main__":
    model, metrics = train_and_evaluate_final()